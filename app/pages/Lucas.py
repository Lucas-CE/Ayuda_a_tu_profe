import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import SimpleJsonOutputParser
import dotenv
import os
import PyPDF2
from io import BytesIO

st.set_page_config(
    page_title="Generador de Evaluaciones",
    page_icon="游닇",
)

# Cargar variables de entorno
dotenv.load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

llm = ChatOpenAI(openai_api_key=api_key, model="gpt-4o-mini")

# Plantilla para el sistema
system_template_message = """
Eres un profesor experto en crear evaluaciones de la materia {topic}.
"""

user_template_message = """
Basado en la siguiente bibliograf칤a:
{bibliography}

en las siguientes preguntas realizadas anteriormente:
{sample_questions}

Crea {question_quantity} preguntas de tipo {question_type}
sobre el tema {topic}, y que las respuestas sean {directness}
a partir de la bibliograf칤a.
"""

output_desarrollo_template = """
El output debe ser un objeto JSON con las llaves
- pregunta_i: la pregunta generada
- bibliografia_i: la secci칩n relevante de la bibliograf칤a
- respuesta_i: la respuesta basada en la bibliograf칤a

Ejemplo:
{{
    "pregunta_1": "쮺u치l es la capital de Francia?",
    "bibliografia_1": "la capital de Francia es Par칤s",
    "respuesta_1": "Par칤s",
    "pregunta_2": "쮺u치l es la capital de Espa침a?",
    "bibliografia_2": "la capital de Espa침a es Madrid",
    "respuesta_2": "Madrid",
}}
"""

output_alternativas_template = """
El output debe ser un objeto JSON con las llaves
- pregunta_i: la pregunta generada
- bibliografia_i: la secci칩n relevante de la bibliograf칤a
- respuesta_i: la respuesta basada en la bibliograf칤a
- alternativas_i: las alternativas de respuesta

Ejemplo:
{
    "pregunta_1": "쮺u치l es la capital de Francia?",
    "bibliografia_1": "la capital de Francia es Par칤s",
    "respuesta_1": "Par칤s",
    "alternativas_1": ["Par칤s", "Madrid", "Londres", "Berl칤n"],
    "pregunta_2": "쮺u치l es la capital de Espa침a?",
    "bibliografia_2": "la capital de Espa침a es Madrid",
    "respuesta_2": "Madrid",
    "alternativas_2": ["Par칤s", "Madrid", "Londres", "Berl칤n"],
}
"""

output_verdadero_falso_template = """
El output debe ser un objeto JSON con las llaves
- pregunta_i: la pregunta generada
- bibliografia_i: la secci칩n relevante de la bibliograf칤a
- respuesta_i: la respuesta basada en la bibliograf칤a

Ejemplo:
{
    "pregunta_1": "쯃a capital de Francia es Par칤s?",
    "bibliografia_1": "la capital de Francia es Par칤s",
    "respuesta_1": "Verdadero",
    "pregunta_2": "쯃a capital de Espa침a es Madrid?",
    "bibliografia_2": "la capital de Espa침a es Madrid",
    "respuesta_2": "Verdadero",
}
"""

# Iniciar variables de estado
if "questions_generated" not in st.session_state:
    st.session_state.questions_generated = []
if "questions_selected" not in st.session_state:
    st.session_state.questions_selected = []


# Funci칩n para leer archivos PDF
def read_pdf(file):
    pdf_reader = PyPDF2.PdfReader(file)
    text = ""
    for page_num in range(len(pdf_reader.pages)):
        text += pdf_reader.pages[page_num].extract_text()
    return text


# Interfaz de Streamlit
st.markdown("# Crea tu evaluaci칩n")

# Selecci칩n de par치metros
topic = st.text_input("Ingresa el tema de la evaluaci칩n")

num_questions = st.number_input(
    "N칰mero de preguntas", min_value=1, max_value=20, step=1
)
question_type = st.selectbox(
    "Tipo de preguntas", ["Alternativas", "Desarrollo", "Verdadero y Falso"]
)

uploaded_bibliography = st.file_uploader("Sube la bibliograf칤a (PDF)", type=["pdf"])
uploaded_sample_questions = st.file_uploader(
    "Sube preguntas anteriores (PDF)", type=["pdf"]
)

directness = st.selectbox(
    "쯈u칠 tan directas deben ser las respuestas a partir de la bibliograf칤a?",
    ["Muy directas", "Semi directas", "No directas"],
)

# Leer la bibliograf칤a y preguntas tipo subidas
bibliography_text = ""
sample_questions_text = ""

if uploaded_bibliography:
    bibliography_text = read_pdf(uploaded_bibliography)

if uploaded_sample_questions:
    sample_questions_text = read_pdf(uploaded_sample_questions)

if uploaded_bibliography and uploaded_sample_questions:
    st.success("Archivos cargados correctamente.")

# Generar preguntas
if (
    st.button("Generar preguntas")
    and uploaded_bibliography
    and uploaded_sample_questions
):
    # Limpiar el estado
    st.session_state.questions_generated = []
    # Seleccionar el template de output
    complete_user_template_message = ""
    if question_type == "Desarrollo":
        complete_user_template_message = (
            user_template_message + output_desarrollo_template
        )
    elif question_type == "Alternativas":
        complete_user_template_message = (
            user_template_message + output_alternativas_template
        )
    elif question_type == "Verdadero y Falso":
        complete_user_template_message = (
            user_template_message + output_verdadero_falso_template
        )
    print("Template:", complete_user_template_message)
    # Crear el prompt para LLM
    prompt_template = ChatPromptTemplate.from_messages(
        messages=[
            ("system", system_template_message),
            ("user", complete_user_template_message),
        ]
    )
    chain = prompt_template | llm | SimpleJsonOutputParser()

    # Crear el input para el modelo
    prompt_input = {
        "bibliography": bibliography_text,
        "sample_questions": sample_questions_text,
        "question_quantity": num_questions,
        "question_type": question_type,
        "directness": directness,
        "topic": topic,
    }

    # Generar las preguntas
    questions_json = chain.invoke(prompt_input)
    st.session_state.questions_generated = questions_json

print("Preguntas generadas:", st.session_state.questions_generated)
# Mostrar las preguntas generadas como "cards"
if st.session_state.questions_generated:
    st.markdown("### Preguntas generadas:")
    # Mostrar cada pregunta con su respuesta y bibliograf칤a
    for question in st.session_state.questions_generated:
        with st.expander(f"**{question['pregunta']}**"):
            st.write(f"Bibliograf칤a: {question['bibliografia']}")
            st.write(f"Respuesta: {question['respuesta']}")
            if "alternativas" in question:
                st.write(f"Alternativas: {', '.join(question['alternativas'])}")

    # Seleccionar preguntas generadas
    selected_questions = st.multiselect(
        "Selecciona las preguntas que deseas agregar al documento final",
        options=[q["pregunta"] for q in st.session_state.questions_generated],
    )


# Funci칩n para generar el PDF
def create_pdf(content):
    pdf_writer = PyPDF2.PdfWriter()
    packet = BytesIO()

    # Crear un archivo PDF en memoria
    pdf_writer.add_blank_page(width=210, height=297)  # A4 dimensions
    packet.seek(0)

    # Escribir el contenido en el archivo
    for page_content in content:
        page = pdf_writer.pages[0]
        page.merge_text(page_content)

    # Guardar el archivo en BytesIO
    pdf_writer.write(packet)
    packet.seek(0)
    return packet


# Bot칩n para generar PDFs
if st.button("Generar PDF") and selected_questions:
    # Generar el contenido de las preguntas seleccionadas
    preguntas_pdf_content = []
    respuestas_pdf_content = []

    for question in st.session_state.questions_generated:
        if question["pregunta"] in selected_questions:
            preguntas_pdf_content.append(f"Pregunta: {question['pregunta']}\n")
            respuestas_pdf_content.append(
                f"Pregunta: {question['pregunta']}\nRespuesta: {question['respuesta']}\n"
            )

    # Crear PDFs para preguntas y respuestas
    preguntas_pdf = create_pdf(preguntas_pdf_content)
    respuestas_pdf = create_pdf(respuestas_pdf_content)

    # Botones para descargar los PDFs
    st.download_button(
        "Descargar preguntas",
        preguntas_pdf,
        file_name="preguntas.pdf",
        mime="application/pdf",
    )
    st.download_button(
        "Descargar pauta", respuestas_pdf, file_name="pauta.pdf", mime="application/pdf"
    )
